import os
import pandas as pd
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import torch
import torch.nn as nn
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)
from utils.logger import print_once

# ì„¤ì •
INPUT_CSV = "../data/refined/dataset.csv"
OUTPUT_CSV = "../data/refined/dataset_sbert_ae.csv"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
WINDOW_SIZE = 30
EMBED_DIM = 384
BOTTLENECK_DIM = 32
BATCH_SIZE = 16
EPOCHS = 300
LR = 1e-3

# 1. ë°ì´í„° ë¡œë”©
df = pd.read_csv(INPUT_CSV)
df["press_titles"] = df["press_titles"].fillna("")
df["date"] = pd.to_datetime(df["date"])
df = df.sort_values("date")
dates = df["date"].unique()

# 2. SBERT + AttentionPooling
sbert = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2", device=DEVICE)

class AttentionPooling(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.attn = nn.Sequential(
            nn.Linear(dim, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )

    def forward(self, x):  # x: (N, D)
        weights = self.attn(x)  # (N, 1)
        weights = torch.softmax(weights, dim=0)
        return (x * weights).sum(dim=0)  # (D,)

attn_pool = AttentionPooling(EMBED_DIM).to(DEVICE)

print("ğŸ“Œ Step 1: ë‚ ì§œë³„ attention pooled ì„ë² ë”© ìƒì„±")
daily_embeds = []
for date in tqdm(dates):
    titles = df[df["date"] == date]["press_titles"].tolist()
    sentences = []
    for t in titles:
        sentences.extend(t.split(" | "))
    if not sentences:
        sentences = [" "]
    embs = sbert.encode(sentences, convert_to_numpy=True)
    embs_tensor = torch.tensor(embs, dtype=torch.float32).to(DEVICE)
    pooled = attn_pool(embs_tensor).detach().cpu().numpy()
    daily_embeds.append(pooled)

X_all = np.stack(daily_embeds)  # shape: (N_dates, 384)
X_tensor = torch.tensor(X_all, dtype=torch.float32).to(DEVICE)

# 3. AE ì •ì˜ ë° sliding window ê¸°ë°˜ í•™ìŠµ
class AE(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(EMBED_DIM, 128),
            nn.ReLU(),
            #nn.Dropout(0.2),
            nn.Linear(128, BOTTLENECK_DIM)
        )
        self.decoder = nn.Sequential(
            nn.Linear(BOTTLENECK_DIM, 128),
            nn.ReLU(),
            nn.Linear(128, EMBED_DIM)
        )

    def forward(self, x):
        z = self.encoder(x)
        recon = self.decoder(z)
        return z, recon

model = AE().to(DEVICE)
if torch.cuda.device_count() > 1:
    print(f"ğŸš€ Using {torch.cuda.device_count()} GPUs")
    model = nn.DataParallel(model)

optimizer = torch.optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss()

# 4. Sliding Window êµ¬ì„±: 30ì¼ í‰ê·  â†’ 31ì¼ì§¸ ë²¡í„° ì˜ˆì¸¡
print("ğŸ“Œ Step 2: Forecast-AEìš© ìœˆë„ìš° êµ¬ì„±")
X_input, X_target = [], []
for i in range(len(X_tensor) - WINDOW_SIZE):
    window = X_tensor[i:i+WINDOW_SIZE]         # (30, 384)
    future = X_tensor[i+WINDOW_SIZE]           # (384,)
    window_vec = window.mean(dim=0)
    X_input.append(window_vec.unsqueeze(0))
    X_target.append(future.unsqueeze(0))

X_input = torch.cat(X_input, dim=0)    # (N, 384)
X_target = torch.cat(X_target, dim=0)  # (N, 384)
print(f"Input shape: {X_input.shape}, Target shape: {X_target.shape}")

# 5. AE í•™ìŠµ
print("ğŸ“Œ Step 3: Forecast-AE í•™ìŠµ ì‹œì‘")
loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_input, X_target), batch_size=BATCH_SIZE, shuffle=True)
model.train()
for epoch in range(EPOCHS):
    total_loss = 0
    for xb, yb in loader:
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
        optimizer.zero_grad()
        z, recon = model(xb)
        loss = criterion(recon, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss / len(loader):.6f}")

# 6. ì„ë² ë”© ìƒì„± (ì˜ˆì¸¡ê¸°ë°˜ z ìƒì„±)
print("ğŸ“Œ Step 4: ë‚ ì§œë³„ ì„ë² ë”© â†’ z(32ì°¨ì›) ìƒì„±")
model.eval()
with torch.no_grad():
    encoder = model.module.encoder if isinstance(model, nn.DataParallel) else model.encoder
    Z = encoder(X_tensor).cpu().numpy()

# 5. ì €ì¥
df_out = pd.DataFrame(Z, columns=[f"press_emb_{i}" for i in range(BOTTLENECK_DIM)])
df_out.insert(0, "date", dates)
df_out.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
print(f"âœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV}")
