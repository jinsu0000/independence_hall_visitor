import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from datetime import datetime
import seaborn as sns

import warnings

warnings.filterwarnings("ignore")

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
import sys
sys.path.append(parent_dir)
from utils.dataset_util import create_sliding_dataset, stratified_timesplit

# ì„¤ì •
WINDOW_SIZE = 30
EPOCHS = 1000
BATCH_SIZE = 64

# ë°ì´í„° ë¡œë”©
DATA_PATH = "../data/refined/dataset.csv"
EMBED_PATH = "../data/refined/dataset_sbert_ae.csv"
output_dir = "../output_FEnML_sbert_mlp_ensemble"
os.makedirs(output_dir, exist_ok=True)

# ë°ì´í„° ë¡œë”© ë° ë³‘í•©
df_main = pd.read_csv(DATA_PATH, parse_dates=["date"]).drop(columns=["press_titles"], errors="ignore")
df_embed = pd.read_csv(EMBED_PATH, parse_dates=["date"])

# ë‚ ì§œ ì •ë ¬ í›„ ìŠ¬ë¼ì´ì‹± ë§ì¶”ê¸°
min_date = df_embed["date"].min()
df_main = df_main[df_main["date"] >= min_date].reset_index(drop=True)

# ë³‘í•©
df = pd.merge(df_main, df_embed, on="date")

# í”¼ì²˜ ë¶„ë¦¬ ë° ì •ê·œí™”
ae_cols = [c for c in df.columns if c.startswith("press_emb")]
base_cols = [c for c in df.columns if c not in ae_cols + ["date", "attendences"]]
df_all = df[["date", "attendences"] + base_cols + ae_cols]
scaler = StandardScaler()
features_scaled = scaler.fit_transform(df_all.drop(columns=["date", "attendences"]))
df_scaled = pd.DataFrame(features_scaled, columns=base_cols + ae_cols)
df_scaled["attendences"] = df_all["attendences"]
df_scaled["date"] = df_all["date"]

# Sliding window dataset
X_seq, y_seq, date_seq = create_sliding_dataset(df_scaled, window=WINDOW_SIZE)
X_mlp = X_seq.reshape(X_seq.shape[0], -1)
date_seq = pd.to_datetime(date_seq)

# Train/Test split
X_train_mlp, X_test_mlp, y_train_raw, y_test_raw, dates_train, dates_test = stratified_timesplit(X_mlp, y_seq, date_seq)

# ğŸ¯ Target ì •ê·œí™”
y_scaler = StandardScaler()
y_train = y_scaler.fit_transform(y_train_raw.reshape(-1, 1)).flatten()
y_test = y_scaler.transform(y_test_raw.reshape(-1, 1)).flatten()

# MLP ëª¨ë¸ ì •ì˜
class DeepMLP(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    def forward(self, x):
        return self.model(x)

# í•™ìŠµ ì¤€ë¹„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = nn.DataParallel(DeepMLP(X_train_mlp.shape[1])).to(device)
loss_fn = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)

train_loader = DataLoader(
    TensorDataset(torch.tensor(X_train_mlp, dtype=torch.float32),
                  torch.tensor(y_train, dtype=torch.float32)),
    batch_size=BATCH_SIZE, shuffle=True
)

# í•™ìŠµ ë£¨í”„
print("ğŸ§  Training MLP...")
for epoch in range(1, EPOCHS + 1):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device).unsqueeze(1)
        pred = model(xb)
        loss = loss_fn(pred, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if epoch % 10 == 0 or epoch == 1:
        print(f"Epoch {epoch:>3} | Loss: {total_loss / len(train_loader):.4f}")

# ì˜ˆì¸¡ ë° ì—­ë³€í™˜
model.eval()
with torch.no_grad():
    y_pred_mlp_scaled = model(torch.tensor(X_test_mlp, dtype=torch.float32).to(device)).cpu().numpy().flatten()
    y_pred_mlp = y_scaler.inverse_transform(y_pred_mlp_scaled.reshape(-1, 1)).flatten()

# ê¸°ì¡´ ML ëª¨ë¸ í•™ìŠµ
models = {
    "LinearRegression": (LinearRegression(), {}, True),
    "DecisionTree": (DecisionTreeRegressor(random_state=42), {"max_depth": [3, 5, 10, None]}, False),
    "RandomForest": (RandomForestRegressor(random_state=42), {"n_estimators": [100], "max_depth": [5, 10]}, False),
    "KNN": (KNeighborsRegressor(), {"n_neighbors": [3, 5, 7]}, True),
    "SVR": (SVR(), {"C": [0.1, 1, 10], "kernel": ["rbf"]}, True),
    "XGBoost": (XGBRegressor(random_state=42), {"n_estimators": [100], "max_depth": [3, 5], "learning_rate": [0.1]}, False)
}

results = []
pred_dict = {}

def evaluate(y_true, y_pred, name="Model"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    r2 = r2_score(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    nrmse = rmse / np.mean(y_true)
    print(f"[{name}] MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.4f}, MAPE={mape:.2f}, NRMSE={nrmse:.4f}")
    return {"Model": name, "MAE": mae, "RMSE": rmse, "R2": r2, "MAPE": mape, "NRMSE": nrmse}

# MLìš© ìŠ¬ë¼ì´ë”© ë°ì´í„° ì¬ìƒì„±
X_ml, y_ml, dates_ml = create_sliding_dataset(df_main, window=WINDOW_SIZE)
dates_ml = pd.to_datetime(dates_ml)

X_train_ml, X_test_ml, y_train_ml, y_test_ml, dates_train_ml, dates_test_ml = stratified_timesplit(X_ml, y_ml, dates_ml)

for name, (model_ml, params, scale) in models.items():
    print(f"ğŸ” Training {name}...")
    if scale:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_ml)
        X_test_scaled = scaler.transform(X_test_ml)
    else:
        X_train_scaled, X_test_scaled = X_train_ml, X_test_ml

    if params:
        grid = GridSearchCV(model_ml, params, cv=TimeSeriesSplit(n_splits=5), scoring="neg_mean_squared_error", n_jobs=-1)
        grid.fit(X_train_scaled, y_train_ml)
        best_model = grid.best_estimator_
    else:
        best_model = model_ml.fit(X_train_scaled, y_train_ml)

    y_pred_ml = best_model.predict(X_test_scaled)
    pred_dict[name] = y_pred_ml
    results.append(evaluate(y_test_ml, y_pred_ml, name))

# MLP í‰ê°€
results.append(evaluate(y_test_ml, y_pred_mlp, "MLP_AE+Base (Window)"))

# ì•™ìƒë¸” í‰ê°€
for name, y_ml in pred_dict.items():
    y_ens = (y_ml + y_pred_mlp) / 2
    results.append(evaluate(y_test_ml, y_ens, f"Ensemble_{name}_+_MLP"))

### RMSE ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
rmse_dict = {
    name: mean_squared_error(y_test_ml, pred_dict[name], squared=False)
    for name in pred_dict
}
inv_rmse = {name: 1 / val for name, val in rmse_dict.items()}
total_weight = sum(inv_rmse.values())
weights = {name: val / total_weight for name, val in inv_rmse.items()}

### Stacking (Meta Regressor)
X_meta = np.column_stack([pred_dict[name] for name in pred_dict] + [y_pred_mlp])
meta_model = Ridge().fit(X_meta, y_test_ml)
y_meta = meta_model.predict(X_meta)
results.append(evaluate(y_test_ml, y_meta, "Ensemble_Stacking_Ridge"))


# ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
df_result = pd.DataFrame(results)
df_result.to_csv(os.path.join(output_dir, "sliding_combined_mlp_results.csv"), index=False)

# ì˜ˆì¸¡ ê³¡ì„ 
plt.figure(figsize=(12, 6))
plt.plot(dates_test_ml, y_test_ml, label="Actual", linestyle="--")
plt.plot(dates_test_ml, y_pred_mlp, label="MLP")
for name in ["XGBoost", "RandomForest"]:
    plt.plot(dates_test_ml, pred_dict[name], label=name)
plt.legend()
plt.title("Prediction Curve")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "sliding_prediction_curve.png"))
plt.close()

# bar plot
plt.figure(figsize=(12, 6))
bar_width = 0.25
x = np.arange(len(df_result["Model"]))
plt.bar(x - bar_width/2, df_result["MAE"], width=bar_width, label="MAE")
plt.bar(x + bar_width/2, df_result["RMSE"], width=bar_width, label="RMSE")
plt.xticks(x, df_result["Model"], rotation=45)
plt.title("Sliding Window Model Performance")
plt.ylabel("Score")
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "sliding_model_comparison.png"))
plt.close()


# ì˜ˆì¸¡ ê³¡ì„  ë¹„êµ
plt.figure(figsize=(14, 6))
plt.plot(dates_test_ml, y_test_ml, label="Actual", linestyle="--", color="black")
plt.plot(dates_test_ml, y_pred_mlp, label="MLP")
plt.plot(dates_test_ml, pred_dict["XGBoost"], label="XGBoost")
plt.plot(dates_test_ml, (pred_dict["XGBoost"] + y_pred_mlp) / 2, label="XGBoost+MLP Avg")
plt.plot(dates_test_ml, y_meta, label="Stacking Ridge")
plt.legend()
plt.title("ğŸ“ˆ Actual vs Predictions")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "prediction_curve_comparison.png"))
plt.close()

# ëª¨ë¸ë³„ ì”ì°¨ ë¶„í¬
plt.figure(figsize=(12, 5))
residuals = {
    "MLP": y_pred_mlp - y_test_ml,
    "XGBoost": pred_dict["XGBoost"] - y_test_ml,
    "Stacking": y_meta - y_test_ml
}
sns.boxplot(data=pd.DataFrame(residuals))
plt.axhline(0, linestyle="--", color="gray")
plt.title("ğŸ“Š Residual Distribution by Model")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "residuals_by_model.png"))
plt.close()

# ëª¨ë¸ë³„ ì„±ëŠ¥ ë°” í”Œë¡¯ (RMSE, MAPE)
fig, ax = plt.subplots(figsize=(14, 6))
bar_width = 0.35
x = np.arange(len(df_result["Model"]))
ax.bar(x - bar_width / 2, df_result["RMSE"], width=bar_width, label="RMSE")
ax.bar(x + bar_width / 2, df_result["MAPE"], width=bar_width, label="MAPE")
ax.set_xticks(x)
ax.set_xticklabels(df_result["Model"], rotation=45, ha="right")
ax.set_title("ğŸ“Š Model Performance (RMSE vs MAPE)")
ax.set_ylabel("Score")
ax.legend()
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "performance_rmse_mape.png"))
plt.close()

# ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ (Stacking ëª¨ë¸ ê¸°ì¤€)
plt.figure(figsize=(6, 6))
plt.scatter(y_test_ml, y_meta, alpha=0.6)
plt.plot([min(y_test_ml), max(y_test_ml)], [min(y_test_ml), max(y_test_ml)], 'k--')
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("ğŸ¯ Actual vs Predicted (Stacking)")
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "scatter_stacking_actual_vs_predicted.png"))
plt.close()